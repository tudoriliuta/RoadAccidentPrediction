{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "# change plotting style to match the theme\n",
    "jtplot.style()\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accidents_data = pd.DataFrame()\n",
    "for i in os.listdir():\n",
    "    if 'Accidents' in i:\n",
    "        accidents_data = accidents_data.append(pd.read_csv(i, low_memory=False))\n",
    "\n",
    "accidents_data['Date'] = pd.to_datetime(accidents_data['Date'])\n",
    "accidents_data = accidents_data[accidents_data['Date'] > '01/01/2012']\n",
    "\n",
    "# Remove entries with missing coordinates\n",
    "accidents_data.dropna(subset = ['Longitude','Latitude'], inplace=True)\n",
    "\n",
    "accidents_data['hours'] = accidents_data['Time'].apply(lambda x: int(str(x)[:2]) if pd.notnull(x) else -1)\n",
    "accidents_data['Month'] = accidents_data['Date'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We take the spatial autocorrelation between the crash events over a geographical space in order to understand the effects of unmeasured confounding variables. KDE has been used in the industry for examining the spatial pattern of crash and identify the hotspots. Other potential approaches could involve K-mean clustering, nearest neighborhood hierarchical clustering (NNH), Moran’s I Index, and Getis-Ord Gi statistics.\n",
    "\n",
    "For this use case we will go forward with sci-kit learn's implementation of KDA since it offers efficiency, adaptability and abstraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "\n",
    "acc_vals = accidents_data[['Latitude', 'Longitude']].values\n",
    "\n",
    "print(\"Spherical Coord KDE\")\n",
    "kde = KernelDensity(bandwidth=1., metric='haversine', kernel='gaussian', algorithm='ball_tree')\n",
    "kde.fit(np.radians(acc_vals))\n",
    "\n",
    "np.exp(kde.score_samples(acc_vals))\n",
    "\n",
    "# score_samples returns the log of the probability density\n",
    "#logprob = kde.score_samples(x_d[:, None])\n",
    "\n",
    "#plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
    "#plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
    "#plt.ylim(-0.02, 0.22)\n",
    "\n",
    "xgrid = accidents_data['Latitude'].values\n",
    "ygrid = accidents_data['Longitude'].values\n",
    "\n",
    "mesh_x, mesh_y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "\n",
    "stack_xy = np.radians(np.vstack([mesh_y.ravel(), mesh_x.ravel()]).T)\n",
    "\n",
    "bandwidths = np.linspace(0.1, 10, 100)\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'), {'bandwidth': bandwidths}, cv=LeaveOneOut(len(acc_vals)))\n",
    "grid.fit(acc_vals)\n",
    "\n",
    "print('Best bandwidth: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "\n",
    "data = fetch_species_distributions()\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "latlon = np.vstack([data.train['dd lat'],\n",
    "                    data.train['dd long']]).T\n",
    "species = np.array([d.decode('ascii').startswith('micro')\n",
    "                    for d in data.train['species']], dtype='int')\n",
    "\n",
    "\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "\n",
    "# Set up the data grid for the contour plot\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "land_mask = (land_reference > -9999).ravel()\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "xy = np.radians(xy[land_mask])\n",
    "\n",
    "# Create two side-by-side plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "cmaps = ['Purples', 'Reds']\n",
    "\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.set_title(species_names[i])\n",
    "    \n",
    "    # plot coastlines with basemap\n",
    "    m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                urcrnrlon=X.max(), resolution='c', ax=axi)\n",
    "    m.drawmapboundary(fill_color='#DDEEFF')\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    \n",
    "    # construct a spherical kernel density estimate of the distribution\n",
    "    kde = KernelDensity(bandwidth=0.03, metric='haversine')\n",
    "    kde.fit(np.radians(latlon[species == i]))\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    Z = np.full(land_mask.shape[0], -9999.0)\n",
    "    Z[land_mask] = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    axi.contourf(X, Y, Z, levels=levels, cmap=cmaps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# if basemap is available, we'll use it.\n",
    "# otherwise, we'll improvise later...\n",
    "try:\n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "    basemap = True\n",
    "except ImportError:\n",
    "    basemap = False\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "data = fetch_species_distributions()\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "\n",
    "Xtrain = np.vstack([data['train']['dd lat'],\n",
    "                    data['train']['dd long']]).T\n",
    "ytrain = np.array([d.decode('ascii').startswith('micro')\n",
    "                  for d in data['train']['species']], dtype='int')\n",
    "Xtrain *= np.pi / 180.  # Convert lat/long to radians\n",
    "\n",
    "# Set up the data grid for the contour plot\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "land_mask = (land_reference > -9999).ravel()\n",
    "\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "xy = xy[land_mask]\n",
    "xy *= np.pi / 180.\n",
    "\n",
    "# Plot map of South America with distributions of each species\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "\n",
    "    # construct a kernel density estimate of the distribution\n",
    "    print(\" - computing KDE in spherical coordinates\")\n",
    "    kde = KernelDensity(bandwidth=0.04, metric='haversine',\n",
    "                        kernel='gaussian', algorithm='ball_tree')\n",
    "    kde.fit(Xtrain[ytrain == i])\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    Z = -9999 + np.zeros(land_mask.shape[0])\n",
    "    Z[land_mask] = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n",
    "\n",
    "    if basemap:\n",
    "        print(\" - plot coastlines using basemap\")\n",
    "        m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                    urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                    urcrnrlon=X.max(), resolution='c')\n",
    "        m.drawcoastlines()\n",
    "        m.drawcountries()\n",
    "    else:\n",
    "        print(\" - plot coastlines from coverage\")\n",
    "        plt.contour(X, Y, land_reference,\n",
    "                    levels=[-9999], colors=\"k\",\n",
    "                    linestyles=\"solid\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.title(species_names[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "\n",
    "data = fetch_species_distributions()\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "latlon = np.vstack([data.train['dd lat'],\n",
    "                    data.train['dd long']]).T\n",
    "species = np.array([d.decode('ascii').startswith('micro')\n",
    "                    for d in data.train['species']], dtype='int')\n",
    "\n",
    "\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "\n",
    "# Set up the data grid for the contour plot\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "land_mask = (land_reference > -9999).ravel()\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "xy = np.radians(xy[land_mask])\n",
    "\n",
    "# Create two side-by-side plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "cmaps = ['Purples', 'Reds']\n",
    "\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.set_title(species_names[i])\n",
    "    \n",
    "    # plot coastlines with basemap\n",
    "    m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                urcrnrlon=X.max(), resolution='c', ax=axi)\n",
    "    m.drawmapboundary(fill_color='#DDEEFF')\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    \n",
    "    # construct a spherical kernel density estimate of the distribution\n",
    "    kde = KernelDensity(bandwidth=0.03, metric='haversine')\n",
    "    kde.fit(np.radians(latlon[species == i]))\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    Z = np.full(land_mask.shape[0], -9999.0)\n",
    "    Z[land_mask] = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    axi.contourf(X, Y, Z, levels=levels, cmap=cmaps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "\n",
    "acc_vals = accidents_data[['Latitude', 'Longitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgrid = accidents_data['Latitude'].values\n",
    "ygrid = accidents_data['Longitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(706947, 706947)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xgrid), len(ygrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh_x, mesh_y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "\n",
    "stack_xy = np.radians(np.vstack([mesh_y.ravel(), mesh_x.ravel()]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandwidths = np.linspace(0.1, 10, 100)\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'), {'bandwidth': bandwidths}, cv=LeaveOneOut(len(acc_vals)))\n",
    "grid.fit(acc_vals)\n",
    "\n",
    "print('Best bandwidth: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coord_vals = accidents_data[['Latitude', 'Longitude']].sample(10000).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap, cm, maskoceans\n",
    "from matplotlib import path\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "\n",
    "def mask_water(points, polys):\n",
    "    \"\"\"\n",
    "    This method masks off the water (where data will be unreliable).\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i, poly in enumerate(polys):\n",
    "        if i == 0:\n",
    "            mask = path.Path(poly).contains_points(points)\n",
    "        else:\n",
    "            mask = mask | path.Path(poly).contains_points(points)\n",
    "    return np.array(mask)\n",
    "\n",
    "def map_data(data, res=.2, dotsize=2, colors=['red', 'yellow', 'blue'], _kds=None):\n",
    "    \n",
    "    min_lat = data['Latitude'].min()\n",
    "    max_lat = data['Latitude'].max()\n",
    "    min_lon = data['Longitude'].min()\n",
    "    max_lon = data['Longitude'].max()\n",
    "\n",
    "    area = 0.1\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
    "    fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "    accident_severity = ['Major', 'Medium', 'Minor']\n",
    "    \n",
    "    for i, axis in enumerate(axes):\n",
    "    \n",
    "        map = Basemap(projection='merc', lat_0 = np.mean([min_lat, max_lat]), lon_0 = np.mean([min_lon, max_lon]),\n",
    "            resolution = 'l', area_thresh = 0.1,\n",
    "            llcrnrlon=min_lon - area, llcrnrlat=min_lat - area,\n",
    "            urcrnrlon=max_lon + area, urcrnrlat=max_lat + area,\n",
    "            epsg=5520)\n",
    "\n",
    "        map.arcgisimage(service='World_Street_Map', xpixels = 200, verbose= True)\n",
    "\n",
    "        #for c in data['Accident_Severity'].unique():\n",
    "        #    lon = data[data['Accident_Severity'] == c]['Longitude'].values\n",
    "        #    lat = data[data['Accident_Severity'] == c]['Latitude'].values\n",
    "        #    x,y = map(lon, lat)\n",
    "        #    map.plot(x, y, 'bo', markersize=dotsize, color=colors[c-1])\n",
    "\n",
    "        model = KernelDensity(bandwidth=3., metric='haversine', kernel='gaussian', algorithm='ball_tree')\n",
    "        model.fit(np.radians(data[data['Accident_Severity'] == i + 1][['Latitude','Longitude']]))\n",
    "\n",
    "        x = np.arange(min_lat, max_lat, res)\n",
    "        y = np.arange(min_lon, max_lon, res)\n",
    "        x_grid, y_grid = np.meshgrid(x, y)\n",
    "        elements_cnt = len(x_grid) * len(x_grid[0, :])\n",
    "\n",
    "        x_unraveled = x_grid.reshape([elements_cnt, 1])\n",
    "        y_unraveled = y_grid.reshape([elements_cnt, 1])\n",
    "        data_to_eval = np.hstack([x_unraveled, y_unraveled])\n",
    "        \n",
    "        density = np.exp(model.score_samples(data_to_eval))#-9999 + np.zeros(data_to_eval.shape[0])\n",
    "        #density = np.exp(model.score_samples(np.radians(data[data['Accident_Severity'] == i + 1][['Latitude','Longitude']])))\n",
    "        # Mask water\n",
    "        x, y = map(data_to_eval[:,1], data_to_eval[:,0])\n",
    "        loc = np.c_[x, y]\n",
    "        polys = [p.boundary for p in map.landpolygons]\n",
    "        on_land = mask_water(loc, polys) \n",
    "        #density[~on_land] = np.exp(model.score_samples(data_to_eval))\n",
    "\n",
    "        #Make predictions using appropriate model. \n",
    "        #density = maskoceans(x_grid, y_grid, density)\n",
    "        density = density.reshape(x_grid.shape)\n",
    "        #density = maskoceans(x_grid, y_grid, density)\n",
    "        axis.contourf(y_grid, x_grid, density, levels=np.linspace(0, density.max(), 25), cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data by label.\n",
    "\n",
    "# For each set, fit a KDE to obtain a generative model of the data. This allows you for any observation xx and label yy to compute a likelihood P(x | y)P(x | y).\n",
    "\n",
    "# From the number of examples of each class in the training set, compute the class prior, P(y)P(y).\n",
    "\n",
    "# For an unknown point xx, the posterior probability for each class is P(y | x)∝P(x | y)P(y)P(y | x)∝P(x | y)P(y). The class which maximizes this posterior is the label assigned to the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_data(accidents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([d.decode('ascii').startswith('micro')\n",
    "                  for d in data['train']['species']], dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# if basemap is available, we'll use it.\n",
    "# otherwise, we'll improvise later...\n",
    "try:\n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "    basemap = True\n",
    "except ImportError:\n",
    "    basemap = False\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "data = fetch_species_distributions()\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "\n",
    "Xtrain = np.vstack([data['train']['dd lat'],\n",
    "                    data['train']['dd long']]).T\n",
    "ytrain = np.array([d.decode('ascii').startswith('micro')\n",
    "                  for d in data['train']['species']], dtype='int')\n",
    "Xtrain *= np.pi / 180.  # Convert lat/long to radians\n",
    "\n",
    "# Set up the data grid for the contour plot\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "land_mask = (land_reference > -9999).ravel()\n",
    "\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "#xy = xy[land_mask]\n",
    "xy *= np.pi / 180.\n",
    "\n",
    "# Plot map of South America with distributions of each species\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "\n",
    "    # construct a kernel density estimate of the distribution\n",
    "    print(\" - computing KDE in spherical coordinates\")\n",
    "    kde = KernelDensity(bandwidth=0.04, metric='haversine',\n",
    "                        kernel='gaussian', algorithm='ball_tree')\n",
    "    kde.fit(Xtrain[ytrain == i])\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    Z = -9999 + np.zeros(land_mask.shape[0])\n",
    "    #Z[land_mask] \n",
    "    Z = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n",
    "\n",
    "    if basemap:\n",
    "        print(\" - plot coastlines using basemap\")\n",
    "        m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                    urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                    urcrnrlon=X.max(), resolution='c')\n",
    "        m.drawcoastlines()\n",
    "        m.drawcountries()\n",
    "    else:\n",
    "        print(\" - plot coastlines from coverage\")\n",
    "        plt.contour(X, Y, land_reference,\n",
    "                    levels=[-9999], colors=\"k\",\n",
    "                    linestyles=\"solid\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.title(species_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from mpl_toolkits.basemap import Basemap, cm, maskoceans\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "latlon = accidents_data[['Latitude','Longitude']]#.as_matrix()\n",
    "res = 1\n",
    "\n",
    "# Create two side-by-side plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "axi = ax[0]\n",
    "\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "\n",
    "x = np.arange(min(accidents_data['Latitude']), max(accidents_data['Latitude']), res)\n",
    "y = np.arange(min(accidents_data['Longitude']), max(accidents_data['Longitude']), res)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "numel = len(X) * len(X[0, :])\n",
    "\n",
    "unraveled_x = X.reshape([numel, 1])\n",
    "unraveled_y = Y.reshape([numel, 1])\n",
    "data_to_eval = np.hstack([unraveled_x, unraveled_y])\n",
    "\n",
    "# construct a spherical kernel density estimate of the distribution\n",
    "kde = KernelDensity(bandwidth=0.03, metric='haversine')\n",
    "kde.fit(np.radians(latlon))\n",
    "\n",
    "m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "            urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "            urcrnrlon=X.max(), resolution='c', ax=axi)\n",
    "\n",
    "m.drawcoastlines()\n",
    "# Mask water\n",
    "#x, y = m(data_to_eval[:,1], data_to_eval[:,0])\n",
    "#loc = np.c_[x, y]\n",
    "#polys = [p.boundary for p in m.landpolygons]\n",
    "#on_land = mask_water(loc, polys) \n",
    "\n",
    "m.drawmapboundary(fill_color='#DDEEFF')\n",
    "m.drawcountries()\n",
    "\n",
    "# evaluate only on the land: -9999 indicates ocean\n",
    "density = np.exp(kde.score_samples(latlon))\n",
    "#density[~on_land] = np.exp(kde.score_samples(latlon))\n",
    "#density = density.reshape(X.shape)\n",
    "\n",
    "data = maskoceans(X, Y, density)\n",
    "# plot contours of the density\n",
    "levels = np.linspace(0, density.max(), 25)\n",
    "axi.contourf(X, Y, data, levels=levels, cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def points_in_polys(points, polys):\n",
    "    result = []\n",
    "    for poly in polys:\n",
    "        mask = nx.points_inside_poly(points, poly)\n",
    "        result.extend(points[mask])\n",
    "        points = points[~mask]\n",
    "    return np.array(result)\n",
    "\n",
    "points = np.random.randint(0, 90, size=(100000, 2))\n",
    "m = Basemap(projection='moll',lon_0=0,resolution='c')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='coral',lake_color='aqua')\n",
    "\n",
    "x, y = m(points[:,0], points[:,1])\n",
    "loc = np.c_[x, y]\n",
    "polys = [p.boundary for p in m.landpolygons]\n",
    "land_loc = points_in_polys(loc, polys)\n",
    "m.plot(land_loc[:, 0], land_loc[:, 1],'ro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kde.score_samples(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In Kernel Density estimation, the bandwidth parameter controls the variance / bias tradeoff. \n",
    "# Therefore, we would want to pick a hyperparameter that is not too narrow and not too wide.\n",
    "# For choosing the optimal value, we test a set of bandwidths through cross validation.\n",
    "bandwidths = [300, 1000]#[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "data = np.radians(coordinates[severity == 1])\n",
    "params = {'bandwidth': bandwidths, 'metric': ['haversine']}\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'), params, cv=LeaveOneOut(len(data)))\n",
    "grid.fit(data)\n",
    "print('Model fit. \\nBest bandwidth value: %s' % grid.best_params_)\n",
    "\n",
    "#kde = KernelDensity(bandwidth=0.03, metric='haversine')\n",
    "#kde.fit(np.radians(coordinates[severity == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from matplotlib import path\n",
    "import random\n",
    "from mpl_toolkits.basemap import Basemap \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeNearestNeighborsDensityPlot(geolocated, res = .2):\n",
    "    #Filter for events with locations. \n",
    "\n",
    "    model = KernelDensity(kernel='gaussian', bandwidth = 3).fit(geolocated[['Latitude','Longitude']])\n",
    "    \n",
    "    #Create a grid of points at which to predict. \n",
    "    min_lat, max_lat = min(geolocated['Latitude']), max(geolocated['Latitude'])\n",
    "    min_lon, max_lon = min(geolocated['Longitude']), max(geolocated['Longitude'])\n",
    "    \n",
    "    x = np.arange(min_lat, max_lat, res)\n",
    "    y = np.arange(min_lon, max_lon, res)\n",
    "    X, Y = meshgrid(x, y)\n",
    "    numel = len(X) * len(X[0, :])\n",
    "    \n",
    "    unraveled_x = X.reshape([numel, 1])\n",
    "    unraveled_y = Y.reshape([numel, 1])\n",
    "    data_to_eval = np.hstack([unraveled_x, unraveled_y])\n",
    "    palettes = ['Purples','Blues']\n",
    "    \n",
    "    #Make predictions using appropriate model. \n",
    "    density = np.exp(model.score_samples(data_to_eval))\n",
    "\n",
    "    #Make map. \n",
    "    figure(figsize = [20, 10])    \n",
    "    m = Basemap(llcrnrlat = min_lat, urcrnrlat = max_lat, llcrnrlon = min_lon, urcrnrlon=max_lon, resolution='l', fix_aspect = False)\n",
    "\n",
    "    # Mask water\n",
    "    #m.drawcoastlines()\n",
    "    #x, y = m(data_to_eval[:,1], data_to_eval[:,0])\n",
    "    #loc = np.c_[x, y]\n",
    "    #polys = [p.boundary for p in m.landpolygons]\n",
    "    #on_land = mask_water(loc, polys) \n",
    "    #density[~on_land] = (color_min + color_max) / 2\n",
    "    \n",
    "    density = density.reshape(X.shape)\n",
    "    contourf(Y, X, density, levels=np.linspace(0, density.max(), 25), cmap=palettes[0])\n",
    "    m.drawcoastlines(linewidth = 2)\n",
    "    m.drawcountries(linewidth=2)\n",
    "    m.drawstates(linewidth = 2)\n",
    "    colorbar()\n",
    "    set_cmap(cmap)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Making plot without normalizing!\")\n",
    "    makeNearestNeighborsDensityPlot(accidents_data[['Latitude','Longitude','Accident_Severity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#          Jake Vanderplas <vanderplas@astro.washington.edu>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "# if basemap is available, we'll use it.\n",
    "# otherwise, we'll improvise later...\n",
    "try:\n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "    basemap = True\n",
    "except ImportError:\n",
    "    basemap = False\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "def create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\n",
    "    \"\"\"Create a bunch with information about a particular organism\n",
    "\n",
    "    This will use the test/train record arrays to extract the\n",
    "    data specific to the given species name.\n",
    "    \"\"\"\n",
    "    bunch = Bunch(name=' '.join(species_name.split(\"_\")[:2]))\n",
    "    species_name = species_name.encode('ascii')\n",
    "    points = dict(test=test, train=train)\n",
    "\n",
    "    for label, pts in points.items():\n",
    "        # choose points associated with the desired species\n",
    "        pts = pts[pts['species'] == species_name]\n",
    "        bunch['pts_%s' % label] = pts\n",
    "\n",
    "        # determine coverage values for each of the training & testing points\n",
    "        ix = np.searchsorted(xgrid, pts['dd long'])\n",
    "        iy = np.searchsorted(ygrid, pts['dd lat'])\n",
    "        bunch['cov_%s' % label] = coverages[:, -iy, ix].T\n",
    "\n",
    "    return bunch\n",
    "\n",
    "\n",
    "def plot_species_distribution(species=(\"bradypus_variegatus_0\",\n",
    "                                       \"microryzomys_minutus_0\")):\n",
    "    \"\"\"\n",
    "    Plot the species distribution.\n",
    "    \"\"\"\n",
    "    if len(species) > 2:\n",
    "        print(\"Note: when more than two species are provided,\"\n",
    "              \" only the first two will be used\")\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    # Load the compressed data\n",
    "    data = fetch_species_distributions()\n",
    "\n",
    "    # Set up the data grid\n",
    "    xgrid, ygrid = construct_grids(data)\n",
    "\n",
    "    # The grid in x,y coordinates\n",
    "    X, Y = np.meshgrid(xgrid, ygrid[::-1])\n",
    "\n",
    "    # create a bunch for each species\n",
    "    BV_bunch = create_species_bunch(species[0],\n",
    "                                    data.train, data.test,\n",
    "                                    data.coverages, xgrid, ygrid)\n",
    "    MM_bunch = create_species_bunch(species[1],\n",
    "                                    data.train, data.test,\n",
    "                                    data.coverages, xgrid, ygrid)\n",
    "\n",
    "    # background points (grid coordinates) for evaluation\n",
    "    np.random.seed(13)\n",
    "    background_points = np.c_[np.random.randint(low=0, high=data.Ny,\n",
    "                                                size=10000),\n",
    "                              np.random.randint(low=0, high=data.Nx,\n",
    "                                                size=10000)].T\n",
    "\n",
    "    # We'll make use of the fact that coverages[6] has measurements at all\n",
    "    # land points.  This will help us decide between land and water.\n",
    "    land_reference = data.coverages[6]\n",
    "\n",
    "    # Fit, predict, and plot for each species.\n",
    "    for i, species in enumerate([BV_bunch, MM_bunch]):\n",
    "        print(\"_\" * 80)\n",
    "        print(\"Modeling distribution of species '%s'\" % species.name)\n",
    "\n",
    "        # Standardize features\n",
    "        mean = species.cov_train.mean(axis=0)\n",
    "        std = species.cov_train.std(axis=0)\n",
    "        train_cover_std = (species.cov_train - mean) / std\n",
    "\n",
    "        # Fit OneClassSVM\n",
    "        print(\" - fit OneClassSVM ... \", end='')\n",
    "        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.5)\n",
    "        clf.fit(train_cover_std)\n",
    "        print(\"done.\")\n",
    "\n",
    "        # Plot map of South America\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        if basemap:\n",
    "            print(\" - plot coastlines using basemap\")\n",
    "            m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                        urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                        urcrnrlon=X.max(), resolution='c')\n",
    "            m.drawcoastlines()\n",
    "            m.drawcountries()\n",
    "        else:\n",
    "            print(\" - plot coastlines from coverage\")\n",
    "            plt.contour(X, Y, land_reference,\n",
    "                        levels=[-9999], colors=\"k\",\n",
    "                        linestyles=\"solid\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        print(\" - predict species distribution\")\n",
    "\n",
    "        # Predict species distribution using the training data\n",
    "        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n",
    "\n",
    "        # We'll predict only for the land points.\n",
    "        idx = np.where(land_reference > -9999)\n",
    "        coverages_land = data.coverages[:, idx[0], idx[1]].T\n",
    "\n",
    "        pred = clf.decision_function((coverages_land - mean) / std)[:, 0]\n",
    "        Z *= pred.min()\n",
    "        Z[idx[0], idx[1]] = pred\n",
    "\n",
    "        levels = np.linspace(Z.min(), Z.max(), 25)\n",
    "        Z[land_reference == -9999] = -9999\n",
    "\n",
    "        # plot contours of the prediction\n",
    "        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n",
    "        plt.colorbar(format='%.2f')\n",
    "\n",
    "        # scatter training/testing points\n",
    "        plt.scatter(species.pts_train['dd long'], species.pts_train['dd lat'],\n",
    "                    s=2 ** 2, c='black',\n",
    "                    marker='^', label='train')\n",
    "        plt.scatter(species.pts_test['dd long'], species.pts_test['dd lat'],\n",
    "                    s=2 ** 2, c='black',\n",
    "                    marker='x', label='test')\n",
    "        plt.legend()\n",
    "        plt.title(species.name)\n",
    "        plt.axis('equal')\n",
    "\n",
    "        # Compute AUC with regards to background points\n",
    "        pred_background = Z[background_points[0], background_points[1]]\n",
    "        pred_test = clf.decision_function((species.cov_test - mean)\n",
    "                                          / std)[:, 0]\n",
    "        scores = np.r_[pred_test, pred_background]\n",
    "        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        plt.text(-35, -70, \"AUC: %.3f\" % roc_auc, ha=\"right\")\n",
    "        print(\"\\n Area under the ROC curve : %f\" % roc_auc)\n",
    "\n",
    "    print(\"\\ntime elapsed: %.2fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "plot_species_distribution()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a predictive model to infer the coordinates from the data\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_lat = accidents['Latitude'].values\n",
    "y_lon = accidents['Longitude'].values\n",
    "\n",
    "x_train_lat, x_test_lat, y_train_lat, y_test_lat = train_test_split(\n",
    "    accidents.drop(['Accident_Index', 'Longitude','Latitude'], axis=1).astype(float), \n",
    "    y_lat, test_size=0.3, random_state=54)\n",
    "\n",
    "x_train_lon, x_test_lon, y_train_lon, y_test_lon = train_test_split(\n",
    "    accidents.drop(['Accident_Index', 'Longitude','Latitude'], axis=1).astype(float), \n",
    "    y_lon, test_size=0.3, random_state=54)\n",
    "\n",
    "d_train_lat = xgb.DMatrix(x_train_lat, y_train_lat)\n",
    "d_train_lon = xgb.DMatrix(x_train_lon, y_train_lon)\n",
    "\n",
    "d_test_lat = xgb.DMatrix(x_test_lat)\n",
    "d_test_lon = xgb.DMatrix(x_test_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':2, 'eta':0.3, 'silent':0, 'objective':'reg:linear', 'eval_metric':'rmse', 'seed':12}\n",
    "model_lat = xgb.train(param, d_train_lat, 10)\n",
    "prediction_lat = model_lat.predict(d_test_lat)\n",
    "prediction_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
